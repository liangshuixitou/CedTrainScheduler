{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/data/l1hy/ali-cluster/cluster-trace-gpu-v2020/data\"\n",
    "\n",
    "def read_csv_with_header(\n",
    "    file_path: str,\n",
    "    header: Optional[list[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"读取 CSV 文件并处理表头\n",
    "\n",
    "    Args:\n",
    "        file_path: CSV 文件路径\n",
    "        header: 可选的表头列表。如果为 None, 则从对应的 .header 文件读取\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 读取并设置好表头的数据框\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: 当 CSV 文件或对应的 header 文件不存在时\n",
    "        pd.errors.EmptyDataError: 当 CSV 文件为空时\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    df.columns = (pd.read_csv(\"{}.header\".format(file_path.split('.csv')[0])).columns\n",
    "                 if header is None else header)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_configs():\n",
    "    \"\"\"获取预定义的模型配置\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"model_name\": \"ResNet50\",\n",
    "            \"task_type\": \"Image Classification\",\n",
    "            \"batch_sizes\": [32, 64, 128],\n",
    "            \"dataset_name\": \"ImageNet\",\n",
    "            \"dataset_size\": 150000,  # ImageNet原始数据集约150GB\n",
    "            \"model_size\": 98,  # ResNet50标准模型大小约98MB\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"MobileNetV3\",\n",
    "            \"task_type\": \"Image Classification\",\n",
    "            \"batch_sizes\": [32, 64, 128],\n",
    "            \"dataset_name\": \"ImageNet\",\n",
    "            \"dataset_size\": 150000,\n",
    "            \"model_size\": 22,  # MobileNetV3轻量化模型典型大小\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"ResNet18\",\n",
    "            \"task_type\": \"Image Classification\",\n",
    "            \"batch_sizes\": [32, 64, 128],\n",
    "            \"dataset_name\": \"CIFAR-10\",\n",
    "            \"dataset_size\": 170,  # CIFAR-10压缩包标准大小约170MB\n",
    "            \"model_size\": 45,  # ResNet18典型参数量对应约45MB\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"MobileNetV2\",\n",
    "            \"task_type\": \"Image Classification\",\n",
    "            \"batch_sizes\": [32, 64, 128],\n",
    "            \"dataset_name\": \"CIFAR-10\",\n",
    "            \"dataset_size\": 170,\n",
    "            \"model_size\": 14,  # MobileNetV2轻量化版本典型大小\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"EfficientNet\",\n",
    "            \"task_type\": \"Image Classification\",\n",
    "            \"batch_sizes\": [32, 64, 128],\n",
    "            \"dataset_name\": \"CIFAR-10\",\n",
    "            \"dataset_size\": 170,\n",
    "            \"model_size\": 29,  # EfficientNet-B0基准模型大小\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"VGG11\",\n",
    "            \"task_type\": \"Image Classification\",\n",
    "            \"batch_sizes\": [32, 64, 128],\n",
    "            \"dataset_name\": \"CIFAR-10\",\n",
    "            \"dataset_size\": 170,\n",
    "            \"model_size\": 507,  # VGG11典型参数量对应约507MB\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"DCGAN\",\n",
    "            \"task_type\": \"Image Generation\",\n",
    "            \"batch_sizes\": [64, 128, 256],\n",
    "            \"dataset_name\": \"LSUN\",\n",
    "            \"dataset_size\": 42000,  # LSUN官方发布版本约42GB\n",
    "            \"model_size\": 45,  # DCGAN基础架构典型模型大小\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"PointNet\",\n",
    "            \"task_type\": \"3D Point Cloud Processing\",\n",
    "            \"batch_sizes\": [32, 64, 128],\n",
    "            \"dataset_name\": \"ShapeNet\",\n",
    "            \"dataset_size\": 30000,  # ShapeNet Core55版本约30GB\n",
    "            \"model_size\": 40,  # PointNet基础模型参数量对应约40MB\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"BERT\",\n",
    "            \"task_type\": \"Question Answering\",\n",
    "            \"batch_sizes\": [32],\n",
    "            \"dataset_name\": \"SQuAD\",\n",
    "            \"dataset_size\": 35000,  # SQuAD v2.0预处理后约35GB\n",
    "            \"model_size\": 1200,  # BERT-Base英文版约1.2GB\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"LSTM\",\n",
    "            \"task_type\": \"Language Modeling\",\n",
    "            \"batch_sizes\": [32, 64, 128],\n",
    "            \"dataset_name\": \"Wikitext2\",\n",
    "            \"dataset_size\": 5000,  # Wikitext2原始文本约5GB\n",
    "            \"model_size\": 35,  # 单层LSTM典型参数量对应约35MB\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"Transformer\",\n",
    "            \"task_type\": \"Machine Translation\",\n",
    "            \"batch_sizes\": [16, 32, 64],\n",
    "            \"dataset_name\": \"Multi30k\",\n",
    "            \"dataset_size\": 3000,  # Multi30k标准版本约3GB\n",
    "            \"model_size\": 85,  # 基础Transformer模型参数量对应约85MB\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_micro_task_configs():\n",
    "    \"\"\"获取微任务的配置\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"model_name\": \"ResNet50\",\n",
    "            \"task_type\": \"Image Classification\",\n",
    "            \"batch_sizes\": [32, 64, 128],\n",
    "            \"dataset_name\": \"ImageNet\",\n",
    "            \"dataset_size\": 150000,  # ImageNet原始数据集约150GB\n",
    "            \"model_size\": 98,  # ResNet50标准模型大小约98MB\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"ResNet18\",\n",
    "            \"task_type\": \"Image Classification\",\n",
    "            \"batch_sizes\": [32, 64, 128],\n",
    "            \"dataset_name\": \"CIFAR-10\",\n",
    "            \"dataset_size\": 170,  # CIFAR-10压缩包标准大小约170MB\n",
    "            \"model_size\": 45,  # ResNet18典型参数量对应约45MB\n",
    "        },\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_training_data(task_df) -> pd.DataFrame:\n",
    "    \"\"\"数据预处理：筛选和清理训练任务数据\n",
    "\n",
    "    对原始任务数据进行预处理，包括：\n",
    "    1. 合并任务和作业数据\n",
    "    2. 清理无效时间戳\n",
    "    3. 筛选有效训练任务\n",
    "    4. 标准化时间\n",
    "    5. 统一GPU类型\n",
    "\n",
    "    Args:\n",
    "        task_df: 任务数据表\n",
    "        job_df: 作业数据表\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: 处理后的训练任务数据，包含以下主要字段：\n",
    "            - job_name: 作业名称\n",
    "            - gpu_type: GPU类型(V100或T4)\n",
    "            - runtime: 运行时长\n",
    "            - norm_job_submit_time: 标准化后的作业提交时间\n",
    "    \"\"\"\n",
    "    # 常量定义\n",
    "    MIN_RUNTIME_SECONDS = 1000.0  # 最小运行时间（秒）\n",
    "    VALID_TASK_TYPES = ['tensorflow', 'PyTorchWorker', 'worker']\n",
    "\n",
    "\n",
    "    # 处理无效的时间戳\n",
    "    task_df.loc[task_df.start_time == 0, ['start_time', 'end_time']] = np.nan\n",
    "    task_df['runtime'] = task_df.end_time - task_df.start_time\n",
    "\n",
    "    # 筛选有效的训练任务\n",
    "    valid_tasks = task_df[\n",
    "        (task_df['status'] == 'Terminated') &             # 已完成的任务\n",
    "        (task_df['gpu_type'] != 'MISC') &                 # 排除杂项GPU类型\n",
    "        (task_df['plan_gpu'] == 100) &                    # 完整GPU使用\n",
    "        (task_df['runtime'] >= MIN_RUNTIME_SECONDS) &     # 运行时间足够长\n",
    "        (task_df['inst_num'] <= 8) &                     # 实例数量小于8\n",
    "        (task_df['task_name'].isin(VALID_TASK_TYPES))     # 有效的任务类型\n",
    "    ]\n",
    "\n",
    "    # 按提交时间排序并标准化\n",
    "    valid_tasks = valid_tasks.sort_values(['start_time'])\n",
    "\n",
    "    # 去重并统一GPU类型名称\n",
    "    valid_tasks.loc[valid_tasks.gpu_type == 'V100M32', 'gpu_type'] = 'V100'\n",
    "\n",
    "    return valid_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def sample_tasks_random(task_df: pd.DataFrame, jobs_count: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    从任务数据中纯随机采样指定数量的任务\n",
    "\n",
    "    Args:\n",
    "        task_df: 任务数据表\n",
    "        jobs_count: 需要采样的任务总数量\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 随机采样后的任务数据表\n",
    "    \"\"\"\n",
    "    # 确保采样数量不超过可用数据量\n",
    "    sample_size = min(jobs_count, len(task_df))\n",
    "\n",
    "    # 直接随机采样\n",
    "    sampled_tasks = task_df.sample(n=sample_size)\n",
    "\n",
    "    # 统计采样结果\n",
    "    single_gpu_count = len(sampled_tasks[sampled_tasks['inst_num'] <= 4])\n",
    "    multi_gpu_count = len(sampled_tasks[sampled_tasks['inst_num'] > 4])\n",
    "\n",
    "    # 打印采样统计信息\n",
    "    print(f\"采样统计: 总任务数 {len(sampled_tasks)}, \"\n",
    "          f\"单卡任务 {single_gpu_count} ({single_gpu_count/len(sampled_tasks):.1%}), \"\n",
    "          f\"多卡任务 {multi_gpu_count} ({multi_gpu_count/len(sampled_tasks):.1%})\")\n",
    "\n",
    "    return sampled_tasks\n",
    "\n",
    "def sample_tasks_with_ratio(task_df: pd.DataFrame, jobs_count: int, multi_gpu_ratio: float = 0.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    从任务数据中采样指定数量的任务，并提高多卡任务的比例\n",
    "\n",
    "    Args:\n",
    "        task_df: 任务数据表\n",
    "        jobs_count: 采样任务总数量\n",
    "        multi_gpu_ratio: 多卡任务在采样中的目标比例，默认为0.5\n",
    "    Returns:\n",
    "        pd.DataFrame: 采样后的任务数据表，多卡任务比例提高\n",
    "    \"\"\"\n",
    "    # 将任务分为单卡和多卡两组\n",
    "    single_gpu_tasks = task_df[task_df['inst_num'] <= 4]\n",
    "    multi_gpu_tasks = task_df[task_df['inst_num'] > 4]\n",
    "\n",
    "    # 计算需要的多卡和单卡任务数量\n",
    "    multi_gpu_count = int(jobs_count * multi_gpu_ratio)\n",
    "    single_gpu_count = jobs_count - multi_gpu_count\n",
    "\n",
    "    # 如果任一组的任务数量不足，调整采样数量\n",
    "    if len(multi_gpu_tasks) < multi_gpu_count:\n",
    "        multi_gpu_count = len(multi_gpu_tasks)\n",
    "        single_gpu_count = jobs_count - multi_gpu_count\n",
    "\n",
    "    if len(single_gpu_tasks) < single_gpu_count:\n",
    "        single_gpu_count = len(single_gpu_tasks)\n",
    "        multi_gpu_count = jobs_count - single_gpu_count\n",
    "\n",
    "    # 分别从两组中采样\n",
    "    sampled_single = single_gpu_tasks.sample(n=single_gpu_count) if single_gpu_count > 0 else pd.DataFrame()\n",
    "    sampled_multi = multi_gpu_tasks.sample(n=multi_gpu_count) if multi_gpu_count > 0 else pd.DataFrame()\n",
    "\n",
    "    # 合并结果\n",
    "    sampled_tasks = pd.concat([sampled_single, sampled_multi])\n",
    "\n",
    "    # 打印采样统计信息\n",
    "    print(f\"采样统计: 总任务数 {len(sampled_tasks)}, 单卡任务 {len(sampled_single)} ({len(sampled_single)/len(sampled_tasks):.1%}), \"\n",
    "          f\"多卡任务 {len(sampled_multi)} ({len(sampled_multi)/len(sampled_tasks):.1%})\")\n",
    "\n",
    "    return sampled_tasks\n",
    "\n",
    "def wrap_task_runtimes(task_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"生成不同 GPU 类型的运行时间\"\"\"\n",
    "    t4_performance = 8.1\n",
    "    p100_performance = 9.3\n",
    "    v100_performance = 15.7\n",
    "\n",
    "    runtimes = {\n",
    "        'T4': (1, 1),\n",
    "        'P100': (t4_performance / p100_performance, t4_performance / p100_performance),\n",
    "        'V100': (t4_performance / v100_performance, t4_performance / v100_performance),\n",
    "    }\n",
    "    gpu_types = runtimes.keys()\n",
    "\n",
    "    def gen_runtime(from_gpu, to_gpu, origin_runtime):\n",
    "        if from_gpu == to_gpu:\n",
    "            return origin_runtime\n",
    "        if from_gpu not in gpu_types:\n",
    "            print(\"not in gpu_types:\", from_gpu)\n",
    "        to_rand = random.uniform(*runtimes[to_gpu])\n",
    "        from_rand = random.uniform(*runtimes[from_gpu])\n",
    "        return int(origin_runtime * to_rand / from_rand)\n",
    "\n",
    "    for gpu_type in gpu_types:\n",
    "        task_df[f'runtime_{gpu_type}'] = task_df.apply(\n",
    "            lambda row, gpu_type=gpu_type: gen_runtime(row['gpu_type'], gpu_type, row['runtime']),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    return task_df\n",
    "\n",
    "def wrap_task_name(task_df: pd.DataFrame, model_configs: list[dict]) -> pd.DataFrame:\n",
    "    # 为每个任务随机分配一个模型\n",
    "    def assign_model(row):\n",
    "        model_config = random.choice(model_configs)\n",
    "        return model_config['model_name']\n",
    "\n",
    "    # 应用模型分配\n",
    "    task_df['task_name'] = task_df.apply(assign_model, axis=1)\n",
    "    return task_df\n",
    "\n",
    "def to_csv(df, name):\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_micro_tasks_random(task_df: pd.DataFrame, jobs_count: int, ratio: float = 0.5) -> pd.DataFrame:\n",
    "    # 将任务分为单卡和多卡两组\n",
    "    single_gpu_tasks = task_df[task_df['inst_num'] == 1]\n",
    "    two_gpu_tasks = task_df[task_df['inst_num'] == 2]\n",
    "\n",
    "    # 计算需要的多卡和单卡任务数量\n",
    "    two_gpu_count = int(jobs_count * ratio)\n",
    "    single_gpu_count = jobs_count - two_gpu_count\n",
    "\n",
    "    # 如果任一组的任务数量不足，调整采样数量\n",
    "    if len(two_gpu_tasks) < two_gpu_count:\n",
    "        two_gpu_count = len(two_gpu_tasks)\n",
    "        single_gpu_count = jobs_count - two_gpu_count\n",
    "\n",
    "    if len(single_gpu_tasks) < single_gpu_count:\n",
    "        single_gpu_count = len(single_gpu_tasks)\n",
    "        two_gpu_count = jobs_count - single_gpu_count\n",
    "\n",
    "    # 分别从两组中采样\n",
    "    sampled_single = single_gpu_tasks.sample(n=single_gpu_count) if single_gpu_count > 0 else pd.DataFrame()\n",
    "    sampled_two = two_gpu_tasks.sample(n=two_gpu_count) if two_gpu_count > 0 else pd.DataFrame()\n",
    "\n",
    "    sampled_micro_tasks = pd.concat([sampled_single, sampled_two])\n",
    "    # 随机分布一下\n",
    "    sampled_micro_tasks = sampled_micro_tasks.sample(frac=1).reset_index(drop=True)\n",
    "    # 随机生成运行时间，<2000s\n",
    "    sampled_micro_tasks['runtime'] = np.random.randint(100, 600, size=len(sampled_micro_tasks))\n",
    "\n",
    "    return sampled_micro_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_task_config(valid_task_df, jobs_count: int, task_type: str):\n",
    "    all_model_configs = get_model_configs()\n",
    "    micro_model_configs = get_micro_task_configs()\n",
    "    \"\"\"生成任务配置\"\"\"\n",
    "    if task_type == \"random\":\n",
    "        random_task_df = sample_tasks_random(valid_task_df, jobs_count)\n",
    "        random_task_wrap_runtimes_df = wrap_task_runtimes(random_task_df)\n",
    "        random_task_wrap_runtimes_df = wrap_task_name(random_task_wrap_runtimes_df, all_model_configs)\n",
    "        to_csv(random_task_wrap_runtimes_df, f\"case_random_{jobs_count}_tasks.csv\")\n",
    "        print(f\"task_file:{f'case_random_{jobs_count}_tasks.csv'} generated\")\n",
    "    elif task_type == \"light\":\n",
    "        light_task_df = sample_tasks_with_ratio(valid_task_df, jobs_count, 0.1)\n",
    "        light_task_wrap_runtimes_df = wrap_task_runtimes(light_task_df)\n",
    "        light_task_wrap_runtimes_df = wrap_task_name(light_task_wrap_runtimes_df, all_model_configs)\n",
    "        to_csv(light_task_wrap_runtimes_df, f\"case_light_{jobs_count}_tasks.csv\")\n",
    "        print(f\"task_file:{f'case_light_{jobs_count}_tasks.csv'} generated\")\n",
    "    elif task_type == \"heavy\":\n",
    "        heavy_task_df = sample_tasks_with_ratio(valid_task_df, jobs_count, 0.5)\n",
    "        heavy_task_wrap_runtimes_df = wrap_task_runtimes(heavy_task_df)\n",
    "        heavy_task_wrap_runtimes_df = wrap_task_name(heavy_task_wrap_runtimes_df, all_model_configs)\n",
    "        to_csv(heavy_task_wrap_runtimes_df, f\"case_heavy_{jobs_count}_tasks.csv\")\n",
    "        print(f\"task_file:{f'case_heavy_{jobs_count}_tasks.csv'} generated\")\n",
    "    elif task_type == \"micro\":\n",
    "        micro_task_df = sample_micro_tasks_random(valid_task_df, jobs_count)\n",
    "        micro_task_wrap_runtimes_df = wrap_task_runtimes(micro_task_df)\n",
    "        micro_task_wrap_runtimes_df = wrap_task_name(micro_task_wrap_runtimes_df, micro_model_configs)\n",
    "        to_csv(micro_task_wrap_runtimes_df, f\"case_micro_{jobs_count}_tasks.csv\")\n",
    "        print(f\"task_file:{f'case_micro_{jobs_count}_tasks.csv'} generated\")\n",
    "\n",
    "def generate_simulator_task_config():\n",
    "    jobs_count_list = [1000, 1300, 1600, 1900, 2200, 2500]\n",
    "\n",
    "    task_df = read_csv_with_header(os.path.join(DATA_DIR, \"pai_task_table.csv\"))\n",
    "    valid_task_df = preprocess_training_data(task_df)\n",
    "\n",
    "    base_jobs_count = 2500\n",
    "    random_base_task_df = sample_tasks_random(valid_task_df, base_jobs_count)\n",
    "    light_base_task_df = sample_tasks_with_ratio(valid_task_df, base_jobs_count, 0.1)\n",
    "    heavy_base_task_df = sample_tasks_with_ratio(valid_task_df, base_jobs_count, 0.5)\n",
    "\n",
    "    for jobs_count in jobs_count_list:\n",
    "        generate_task_config(random_base_task_df, jobs_count, \"random\")\n",
    "        generate_task_config(light_base_task_df, jobs_count, \"light\")\n",
    "        generate_task_config(heavy_base_task_df, jobs_count, \"heavy\")\n",
    "\n",
    "def generate_micro_task_config():\n",
    "    jobs_count_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "    task_df = read_csv_with_header(os.path.join(DATA_DIR, \"pai_task_table.csv\"))\n",
    "    valid_task_df = preprocess_training_data(task_df)\n",
    "    for jobs_count in jobs_count_list:\n",
    "        generate_task_config(valid_task_df, jobs_count, \"micro\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # generate_simulator_task_config()\n",
    "    generate_micro_task_config()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ced-train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
