{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "\n",
    "def generate_node(node_type):\n",
    "    \"\"\"生成单个节点的配置\"\"\"\n",
    "    # GPU配置（数量为2的整数倍）\n",
    "    gpu_configs = {\n",
    "        \"cloud\": [4, 8],      # 云端节点4或8个GPU\n",
    "        \"edge\": [2, 4],       # 边缘节点2或4个GPU\n",
    "        \"terminal\": [1, 2]    # 终端节点1或2个GPU\n",
    "    }\n",
    "\n",
    "    # CPU配置（按实际服务器配置，8的整数倍）\n",
    "    cpu_configs = {\n",
    "        \"cloud\": [32, 48, 64],     # 云端节点CPU核心数\n",
    "        \"edge\": [16, 24, 32],      # 边缘节点CPU核心数\n",
    "        \"terminal\": [8, 16, 24]    # 终端节点CPU核心数\n",
    "    }\n",
    "\n",
    "    # 内存配置（按实际服务器配置，16的整数倍，单位GB）\n",
    "    memory_configs = {\n",
    "        \"cloud\": [128, 256, 512],    # 云端节点内存\n",
    "        \"edge\": [64, 128, 256],      # 边缘节点内存\n",
    "        \"terminal\": [32, 64, 128]    # 终端节点内存\n",
    "    }\n",
    "\n",
    "    # GPU型号及其对应的显存配置\n",
    "    gpu_specs = {\n",
    "        \"cloud\": {\"model\": \"V100\", \"memory\": 32},      # V100 32GB\n",
    "        \"edge\": {\"model\": \"P100\", \"memory\": 16},       # P100 16GB\n",
    "        \"terminal\": {\"model\": \"T4\", \"memory\": 16}      # T4 16GB\n",
    "    }\n",
    "\n",
    "    gpu_count = random.choice(gpu_configs[node_type])\n",
    "    gpu_model = gpu_specs[node_type][\"model\"]\n",
    "\n",
    "    gpu_list = []\n",
    "    for _ in range(gpu_count):\n",
    "        gpu_list.append(\n",
    "            {\n",
    "                \"gpu_id\": str(uuid.uuid4()),\n",
    "                \"gpu_type\": gpu_model,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"node_id\": f\"{node_type}_node_{random.randint(1000, 9999)}\",\n",
    "        \"cpu_cores\": random.choice(cpu_configs[node_type]),\n",
    "        \"memory\": random.choice(memory_configs[node_type]),\n",
    "        \"gpu_count\": gpu_count,\n",
    "        \"gpu_model\": gpu_model,\n",
    "        \"gpus\": gpu_list,\n",
    "        \"gpu_memory\": gpu_specs[node_type][\"memory\"],\n",
    "        \"ip_address\": f\"192.168.{random.randint(1, 255)}.{random.randint(1, 255)}\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cluster_config():\n",
    "    \"\"\"生成完整的集群配置\"\"\"\n",
    "    # 集群数量配置\n",
    "    cluster_counts = {\n",
    "        \"cloud\": {\"min\": 1, \"max\": 2},  # 云端集群数量范围\n",
    "        \"edge\": {\"min\": 2, \"max\": 3},  # 边缘集群数量范围\n",
    "        \"terminal\": {\"min\": 3, \"max\": 4},  # 终端集群数量范围\n",
    "    }\n",
    "\n",
    "    # 每个集群的节点数量配置\n",
    "    nodes_per_cluster = {\n",
    "        \"cloud\": {\"min\": 10, \"max\": 15},  # 每个云端集群的节点数量范围\n",
    "        \"edge\": {\"min\": 4, \"max\": 8},  # 每个边缘集群的节点数量范围\n",
    "        \"terminal\": {\"min\": 2, \"max\": 4},  # 每个终端集群的节点数量范围\n",
    "    }\n",
    "\n",
    "    # 带宽配置（单位：Mbps）\n",
    "    intra_domain_bandwidth = {\n",
    "        \"cloud\": {\"min\": 40000, \"max\": 100000},  # 云端集群内带宽范围 (40Gbps-100Gbps)\n",
    "        \"edge\": {\"min\": 10000, \"max\": 40000},    # 边缘集群内带宽范围 (10Gbps-40Gbps)\n",
    "        \"terminal\": {\"min\": 1000, \"max\": 10000}, # 终端集群内带宽范围 (1Gbps-10Gbps)\n",
    "    }\n",
    "\n",
    "    inter_domain_bandwidth = {\"min\": 200, \"max\": 300}  # 域间带宽\n",
    "\n",
    "    cluster_config = {\"clusters\": {\"cloud_clusters\": [], \"edge_clusters\": [], \"terminal_clusters\": []}}\n",
    "\n",
    "    # 生成云端集群\n",
    "    for i in range(random.randint(cluster_counts[\"cloud\"][\"min\"], cluster_counts[\"cloud\"][\"max\"])):\n",
    "        cloud_cluster = {\n",
    "            \"cluster_id\": f\"cloud_cluster_{i+1}\",\n",
    "            \"cluster_name\": f\"CloudCluster-{i+1}\",\n",
    "            \"cluster_type\": \"cloud\",\n",
    "            \"nodes\": [\n",
    "                generate_node(\"cloud\")\n",
    "                for _ in range(random.randint(nodes_per_cluster[\"cloud\"][\"min\"], nodes_per_cluster[\"cloud\"][\"max\"]))\n",
    "            ],\n",
    "            \"intra_domain_bandwidth\": random.randint(\n",
    "                intra_domain_bandwidth[\"cloud\"][\"min\"], intra_domain_bandwidth[\"cloud\"][\"max\"]\n",
    "            ),\n",
    "            \"inter_domain_bandwidth\": random.randint(inter_domain_bandwidth[\"min\"], inter_domain_bandwidth[\"max\"]),\n",
    "        }\n",
    "        cluster_config[\"clusters\"][\"cloud_clusters\"].append(cloud_cluster)\n",
    "\n",
    "    # 生成边缘集群\n",
    "    for i in range(random.randint(cluster_counts[\"edge\"][\"min\"], cluster_counts[\"edge\"][\"max\"])):\n",
    "        edge_cluster = {\n",
    "            \"cluster_id\": f\"edge_cluster_{i+1}\",\n",
    "            \"cluster_name\": f\"EdgeCluster-{i+1}\",\n",
    "            \"cluster_type\": \"edge\",\n",
    "            \"nodes\": [\n",
    "                generate_node(\"edge\")\n",
    "                for _ in range(random.randint(nodes_per_cluster[\"edge\"][\"min\"], nodes_per_cluster[\"edge\"][\"max\"]))\n",
    "            ],\n",
    "            \"intra_domain_bandwidth\": random.randint(\n",
    "                intra_domain_bandwidth[\"edge\"][\"min\"], intra_domain_bandwidth[\"edge\"][\"max\"]\n",
    "            ),\n",
    "            \"inter_domain_bandwidth\": random.randint(inter_domain_bandwidth[\"min\"], inter_domain_bandwidth[\"max\"]),\n",
    "        }\n",
    "        cluster_config[\"clusters\"][\"edge_clusters\"].append(edge_cluster)\n",
    "\n",
    "    # 生成终端集群\n",
    "    for i in range(random.randint(cluster_counts[\"terminal\"][\"min\"], cluster_counts[\"terminal\"][\"max\"])):\n",
    "        terminal_cluster = {\n",
    "            \"cluster_id\": f\"terminal_cluster_{i+1}\",\n",
    "            \"cluster_name\": f\"TerminalCluster-{i+1}\",\n",
    "            \"cluster_type\": \"terminal\",\n",
    "            \"nodes\": [\n",
    "                generate_node(\"terminal\")\n",
    "                for _ in range(\n",
    "                    random.randint(nodes_per_cluster[\"terminal\"][\"min\"], nodes_per_cluster[\"terminal\"][\"max\"])\n",
    "                )\n",
    "            ],\n",
    "            \"intra_domain_bandwidth\": random.randint(\n",
    "                intra_domain_bandwidth[\"terminal\"][\"min\"], intra_domain_bandwidth[\"terminal\"][\"max\"]\n",
    "            ),\n",
    "            \"inter_domain_bandwidth\": random.randint(inter_domain_bandwidth[\"min\"], inter_domain_bandwidth[\"max\"]),\n",
    "        }\n",
    "        cluster_config[\"clusters\"][\"terminal_clusters\"].append(terminal_cluster)\n",
    "\n",
    "    return cluster_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fs_config(cluster_config, version=\"light\"):\n",
    "    \"\"\"生成训练任务配置\"\"\"\n",
    "    # 定义训练任务信息\n",
    "    if version == \"light\":\n",
    "        training_tasks = {\n",
    "            \"ResNet50\": {\n",
    "                \"task_type\": \"Image Classification\",\n",
    "                \"batch_sizes\": [16, 32, 64],\n",
    "                \"dataset_size\": 5000,  # 5GB (CIFAR/部分ImageNet)\n",
    "                \"model_size\": 98  # 98MB\n",
    "            },\n",
    "            \"VGG19\": {\n",
    "                \"task_type\": \"Image Classification\",\n",
    "                \"batch_sizes\": [16, 32],\n",
    "                \"dataset_size\": 6000,  # 6GB (部分数据集)\n",
    "                \"model_size\": 549  # 549MB\n",
    "            },\n",
    "            \"InceptionV3\": {\n",
    "                \"task_type\": \"Image Classification\",\n",
    "                \"batch_sizes\": [16, 32],\n",
    "                \"dataset_size\": 8000,  # 8GB (部分数据集)\n",
    "                \"model_size\": 92  # 92MB\n",
    "            },\n",
    "            \"DenseNet161\": {\n",
    "                \"task_type\": \"Image Classification\",\n",
    "                \"batch_sizes\": [8, 16],\n",
    "                \"dataset_size\": 4000,  # 4GB (部分数据集)\n",
    "                \"model_size\": 110  # 110MB\n",
    "            },\n",
    "            \"DCGAN\": {\n",
    "                \"task_type\": \"Image-to-Image Translation\",\n",
    "                \"batch_sizes\": [64, 128, 256],\n",
    "                \"dataset_size\": 3000,  # 3GB (如CelebA数据集)\n",
    "                \"model_size\": 45  # 45MB\n",
    "            },\n",
    "            \"LSTM\": {\n",
    "                \"task_type\": \"Language Modeling\",\n",
    "                \"batch_sizes\": [10, 20, 40, 80],\n",
    "                \"dataset_size\": 1500,  # 1.5GB (如Wikipedia子集)\n",
    "                \"model_size\": 35  # 35MB\n",
    "            },\n",
    "            \"Transformer\": {\n",
    "                \"task_type\": \"Language Modeling\",\n",
    "                \"batch_sizes\": [16, 32, 64, 128],\n",
    "                \"dataset_size\": 3500,  # 3.5GB (小型语言数据集)\n",
    "                \"model_size\": 550  # 550MB\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        training_tasks = {\n",
    "            \"ResNet50\": {\n",
    "                \"task_type\": \"Image Classification\",\n",
    "                \"batch_sizes\": [16, 32, 64],\n",
    "                \"dataset_size\": 150000,  # 15GB (完整ImageNet)\n",
    "                \"model_size\": 980  # 980MB (含预训练权重)\n",
    "            },\n",
    "            \"VGG19\": {\n",
    "                \"task_type\": \"Image Classification\",\n",
    "                \"batch_sizes\": [16, 32],\n",
    "                \"dataset_size\": 200000,  # 20GB (大规模数据集)\n",
    "                \"model_size\": 5490  # 5.49GB (含预训练权重)\n",
    "            },\n",
    "            \"InceptionV3\": {\n",
    "                \"task_type\": \"Image Classification\",\n",
    "                \"batch_sizes\": [16, 32],\n",
    "                \"dataset_size\": 180000,  # 18GB (完整数据集)\n",
    "                \"model_size\": 920  # 920MB\n",
    "            },\n",
    "            \"DenseNet161\": {\n",
    "                \"task_type\": \"Image Classification\",\n",
    "                \"batch_sizes\": [8, 16],\n",
    "                \"dataset_size\": 120000,  # 12GB (完整数据集)\n",
    "                \"model_size\": 1100  # 1.1GB\n",
    "            },\n",
    "            \"DCGAN\": {\n",
    "                \"task_type\": \"Image-to-Image Translation\",\n",
    "                \"batch_sizes\": [64, 128, 256],\n",
    "                \"dataset_size\": 80000,  # 8GB (高分辨率图像数据集)\n",
    "                \"model_size\": 450  # 450MB\n",
    "            },\n",
    "            \"LSTM\": {\n",
    "                \"task_type\": \"Language Modeling\",\n",
    "                \"batch_sizes\": [10, 20, 40, 80],\n",
    "                \"dataset_size\": 50000,  # 5GB (大型文本语料库)\n",
    "                \"model_size\": 350  # 350MB\n",
    "            },\n",
    "            \"Transformer\": {\n",
    "                \"task_type\": \"Language Modeling\",\n",
    "                \"batch_sizes\": [16, 32, 64, 128],\n",
    "                \"dataset_size\": 300000,  # 30GB (大型语言模型数据集)\n",
    "                \"model_size\": 5500  # 5.5GB (中等规模Transformer)\n",
    "            }\n",
    "        }\n",
    "    # 获取所有节点的ID\n",
    "    all_nodes = []\n",
    "    for clusters in cluster_config[\"clusters\"].values():\n",
    "        for cluster in clusters:\n",
    "            for node in cluster[\"nodes\"]:\n",
    "                all_nodes.append(node[\"node_id\"])\n",
    "\n",
    "    # 为每个训练任务的数据随机分配节点\n",
    "    training_distribution = {\n",
    "        \"tasks\": []\n",
    "    }\n",
    "\n",
    "    for model_name, task_info in training_tasks.items():\n",
    "        data_nodes = random.sample(all_nodes, 2)\n",
    "        model_nodes = random.sample(all_nodes, 2)\n",
    "\n",
    "        task_config = {\n",
    "            \"model_name\": model_name,\n",
    "            \"task_type\": task_info[\"task_type\"],\n",
    "            \"batch_sizes\": task_info[\"batch_sizes\"],\n",
    "            \"dataset\": {\n",
    "                \"size_mb\": task_info[\"dataset_size\"],\n",
    "                \"storage_nodes\": data_nodes\n",
    "            },\n",
    "            \"model\": {\n",
    "                \"size_mb\": task_info[\"model_size\"],\n",
    "                \"storage_nodes\": model_nodes\n",
    "            }\n",
    "        }\n",
    "        training_distribution[\"tasks\"].append(task_config)\n",
    "\n",
    "    return training_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 生成集群资源配置\n",
    "    cluster_config = generate_cluster_config()\n",
    "\n",
    "    # 生成数据布局配置\n",
    "    light_fs_config = generate_fs_config(cluster_config, version=\"light\")\n",
    "    heavy_fs_config = generate_fs_config(cluster_config, version=\"heavy\")\n",
    "\n",
    "    # 将配置保存到文件\n",
    "    with open(\"cluster_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cluster_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 保存训练任务配置\n",
    "    with open(\"fs_config_light.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(light_fs_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    with open(\"fs_config_heavy.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(heavy_fs_config, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ced-train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
